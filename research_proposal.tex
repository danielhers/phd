\documentclass[11pt]{article}
\usepackage{cite}

\begin{document}

\title{Research Proposal}
\author{Daniel Hershcovich}
\date{\today}
\maketitle

\section{Background}

Deep learning in artificial neural networks has received much attention in several fields of computer science, including computer vision, speech recognition and natural language processing, due to their best performance in various tasks. Several different architectures have come to be included under the term “deep learning”, among them multilayer perceptrons (MLPs), deep belief networks (DBNs), convolutional neural networks (CNNs), and recursive neural networks (RNNs). In natural language processing, many of the methods rely strongly on word embeddings—representation of words as vectors in a continuous vector space with tens or hundreds of dimensions, such that linguistic and semantic regularities between the words are captured in the vectors. This kind of representation can be learned in an unsupervised manner, from a large unlabeled corpus. Moreover, RNNs have been shown to be able to learn semantically meaningful continuous vector representations of multi-word phrases, in the same space. Approaches based on RNNs have achieved state-of-the-art performance in syntactic parsing and sentiment analysis.

Semantic annotation schemes represent the meaning of natural language. An example is semantic role labeling, which annotates predicates and their arguments, classifying them into specific roles. As opposed to syntactic annotation, which reflects language-specific formal patterns, semantic annotation corresponds to a higher level of cognitive processing, and the same framework can potentially apply to any language. Moreover, a rich semantic annotation scheme may be more beneficial as an input for applications that attempt to solve a semantic task, such as machine translation, word sense disambiguation and sentiment analysis.


\section{Aims}

I will use RNNs to learn a recently introduced semantic annotation scheme, Universal Conceptual Cognitive Annotation (UCCA)~\cite{abend2013universal}. This scheme takes a semantic approach to grammatical representation, describing the relations between different parts of the text. Due to the success of RNNs in learning composite linguistic structures, it seems promising that they can successfully learn a semantic grammatical annotation like UCCA.


\section{Techniques}

\subsection{Distributed Representations}
Recursive Autoencoders, RAAM
\subsection{Backpropagation Through Structure (BPTS)}
\cite{goller1996learning} describe a method for learning distributed continuous representation tuned for a supervised task, using a recursive "folding" architecture. The structure can be a general Directed Acyclic Graph (DAG), and is not limited to a tree or sequence. \cite{sperduti1997supervised} train supervised recursive neural networks for the classification of structures. In both cases, 
\subsection{Recursive Neural Networks}
DT-RNNs


\bibliography{research_proposal}{}
\bibliographystyle{plain}
\end{document}
